# MistralStream

## `MistralStream(res: Response, cb?: AIStreamCallbacks): ReadableStream` [#mistralstream]

`MistralStream` is a utility function that transforms the response from Mistral's chat models into a `ReadableStream`.

This works with the official [Mistral API](https://mistral.ai/), and it's supported in both Node.js, the [Edge Runtime](https://edge-runtime.vercel.app), and browser environments.

## Parameters [#parameters]

### `res: Response`

This is the response object returned by the `mistral.chatStream`.

### `cb?: AIStreamCallbacks`

This is an optional parameter which is an object that contains callback functions for handling the start, completion, and each token of the AI response. If not provided, default behavior is applied.

## Examples [#examples]

Below are some examples of how to use `MistralStream`.

### Chat Model Example

```tsx filename="app/api/chat/route.ts" showLineNumbers
import MistralClient from '@mistralai/mistralai';
import { MistralStream, StreamingTextResponse } from 'ai';

export const runtime = 'edge';

const mistral = new MistralClient(process.env.MISTRAL_API_KEY || '');

export async function POST(req: Request) {
  // Extract the `messages` from the body of the request
  const { messages } = await req.json();

  const response = mistral.chatStream({
    model: 'mistral-small',
    maxTokens: 1000,
    messages,
  });

  // Convert the response into a friendly text-stream. The Mistral client responses are
  // compatible with the Vercel AI SDK MistralStream adapter.
  const stream = MistralStream(response);

  // Respond with the stream
  return new StreamingTextResponse(stream);
}
```

### Completion Model Example

```tsx filename="app/api/completion/route.ts" showLineNumbers
import MistralClient from '@mistralai/mistralai';
import {
  MistralStream,
  StreamingTextResponse,
  experimental_StreamData,
} from 'ai';

const mistral = new MistralClient(process.env.MISTRAL_API_KEY || '');

export const runtime = 'edge';

export async function POST(req: Request) {
  // Extract the `prompt` from the body of the request
  const { prompt } = await req.json();

  // Ask Mistral for a streaming completion given the prompt
  const response = mistral.chatStream({
    model: 'mistral-small',
    maxTokens: 1000,
    messages: [{ role: 'user', content: prompt }],
  });

  // optional: use stream data
  const data = new experimental_StreamData();

  data.append({ test: 'value' });

  // Convert the response into a friendly text-stream
  const stream = MistralStream(response, {
    onFinal(completion) {
      data.close();
    },
    experimental_streamData: true,
  });

  // Respond with the stream
  return new StreamingTextResponse(stream, {}, data);
}
```

In these examples, the `MistralStream` function is used to transform the response from Mistral API calls into a `ReadableStream`, allowing the client to consume the AI output as it is generated, rather than waiting for the entire response.
